{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"open_flamingo\")\n",
    "directory_path = os.path.abspath(os.path.join('..'))\n",
    "if directory_path not in sys.path:\n",
    "    sys.path.append(directory_path)\n",
    "from datasets import COCOFlickrDataset, ImageNetDataset\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from training.scheduler import cosine_lr\n",
    "from torchvision import transforms\n",
    "from open_flamingo.eval.classification_utils import IMAGENET_1K_CLASS_ID_TO_LABEL\n",
    "from train.pgd_train import pgd\n",
    "from train.apgd_train import apgd_train as apgd\n",
    "import wandb\n",
    "from utils import init_wandb, AverageMeter\n",
    "from sam_data import SamData\n",
    "from open_flamingo.eval.models.utils import unwrap_model\n",
    "from train.utils import str2bool\n",
    "from CLIP_eval.eval_utils import load_clip_model\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_device = 'cuda:0'\n",
    "eps= 2/255\n",
    "stepsize_adv= 2/255\n",
    "batch_size=64\n",
    "data_path=\"C:/CodesSpring24/Data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC\"\n",
    "inner_loss='l2'\n",
    "norm='Linf'\n",
    "iterations_adv=10\n",
    "stepsize_adv=1.\n",
    "clean_weight=0.\n",
    "momentum_sgd=0.9\n",
    "template='std'\n",
    "clip_model_name='ViT-L-14'\n",
    "output_normalize=False\n",
    "attack='apgd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_orig, _, image_processor = open_clip.create_model_and_transforms(\n",
    "        clip_model_name, pretrained='openai'\n",
    "    )\n",
    "\n",
    "model, _, _ = load_clip_model(clip_model_name, 'openai')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipVisionModel(torch.nn.Module):\n",
    "    def __init__(self, model, args, normalize):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, vision, output_normalize):\n",
    "        embedding = self.model(self.normalize(vision))\n",
    "        if output_normalize:\n",
    "            embedding = F.normalize(embedding, dim=-1)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_without_normalize = transforms.Compose(image_processor.transforms[:-1])\n",
    "normalize = image_processor.transforms[-1]\n",
    "del image_processor\n",
    "print(f'[preprocessor_without_normalize] {preprocessor_without_normalize}')\n",
    "print(f'[normalize] {normalize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageNetDataset(\n",
    "            root=data_path+ '/train',\n",
    "            transform=preprocessor_without_normalize,\n",
    "        ) \n",
    "dataset_eval = ImageNetDataset(\n",
    "        root=data_path + '/val',\n",
    "        transform=preprocessor_without_normalize,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "dataloader_eval = DataLoader(dataset_eval, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)\n",
    "\n",
    "if template == 'std':\n",
    "        template = 'This is a photo of a {}'\n",
    "elif template == 'blurry':\n",
    "        template = 'This is a blurry photo of a {}'\n",
    "else:\n",
    "    raise ValueError(f'Unknown template: {template}')\n",
    "    \n",
    "print(f'template: {template}')\n",
    "texts = [template.format(c) for c in IMAGENET_1K_CLASS_ID_TO_LABEL.values()]\n",
    "print(\"These are samples\",texts[:10])\n",
    "text_tokens = open_clip.tokenize(texts)\n",
    "\n",
    "model_orig.to(main_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        embedding_text_labels_norm = []\n",
    "        for el in (text_tokens[:500], text_tokens[500:]):\n",
    "            # we need to split the text tokens into two batches because otherwise we run out of memory\n",
    "            # note that we are accessing the model directly here, not the CustomModel wrapper\n",
    "            # thus its always normalizing the text embeddings\n",
    "            embedding_text_labels_norm.append(\n",
    "                model_orig.encode_text(el.to(main_device), normalize=True).detach().cpu()\n",
    "            )\n",
    "        embedding_text_labels_norm = torch.cat(embedding_text_labels_norm).T.to(main_device)\n",
    "        assert torch.allclose(\n",
    "            F.normalize(embedding_text_labels_norm, dim=0),\n",
    "            embedding_text_labels_norm\n",
    "        )\n",
    "        if clip_model_name == 'ViT-B-32':\n",
    "            assert embedding_text_labels_norm.shape == (512, 1000), embedding_text_labels_norm.shape\n",
    "        elif clip_model_name in ('ViT-L-14', 'ViT-L-14-336'):\n",
    "            assert embedding_text_labels_norm.shape == (768, 1000), embedding_text_labels_norm.shape\n",
    "        else:\n",
    "            raise ValueError(f'Unknown model: {clip_model_name}')\n",
    "#==========================================================================\n",
    "model_orig.cpu()\n",
    "model_orig = ClipVisionModel(model=model_orig.visual, args=\"args\", normalize=normalize)\n",
    "\n",
    "model = ClipVisionModel(model=model.visual, args=\"args\", normalize=normalize)\n",
    "\n",
    "\n",
    "    # set optimizer (all params have requires_grad=True)\n",
    "params = unwrap_model(model).model.parameters() #unwrap model before saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(out, targets, reduction='none'):\n",
    "\n",
    "    assert out.shape == targets.shape, f'{out.shape} != {targets.shape}'\n",
    "    assert out.shape[0] > 1\n",
    "    # Compute the element-wise squared error\n",
    "    squared_error_batch = F.mse_loss(out, targets, reduction='none')\n",
    "    if reduction == 'mean':\n",
    "        squared_error_batch = torch.mean(squared_error_batch.sum(dim=1))\n",
    "    else:\n",
    "        squared_error_batch = squared_error_batch.sum(dim=1)\n",
    "        assert squared_error_batch.shape == (out.shape[0],), f'{squared_error_batch.shape} != {(out.shape[0],)}'\n",
    "    return squared_error_batch\n",
    "\n",
    "def ce(out, targets, reduction='mean'):\n",
    "    # out = logits\n",
    "    assert out.shape[0] == targets.shape[0], (out.shape, targets.shape)\n",
    "    assert out.shape[0] > 1\n",
    "\n",
    "    return F.cross_entropy(out, targets, reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(loss_str, embedding, targets, embedding_orig, logit_scale,\n",
    "                 embedding_text_labels_norm=None, reduction='mean'):\n",
    "    if loss_str == 'l2':\n",
    "        loss = l2(out=embedding, targets=embedding_orig, reduction=reduction)\n",
    "    elif loss_str == 'ce':\n",
    "        loss = ce(\n",
    "            out=embedding @ (logit_scale * embedding_text_labels_norm),\n",
    "            targets=targets,\n",
    "            reduction=reduction\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f'loss {loss_str} not supported')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLossWrapper:\n",
    "    def __init__(self, embedding_orig, embedding_text_labels_norm, reduction='mean', loss=None,\n",
    "                 logit_scale=100.):\n",
    "        self.embedding_orig = embedding_orig\n",
    "        self.embedding_text_labels_norm = embedding_text_labels_norm\n",
    "        self.reduction = reduction\n",
    "        self.loss_str = loss\n",
    "        self.logit_scale = logit_scale\n",
    "\n",
    "    def __call__(self, embedding, targets):\n",
    "        return compute_loss(\n",
    "            loss_str=self.loss_str, embedding=embedding, targets=targets,\n",
    "            embedding_orig=self.embedding_orig, logit_scale=self.logit_scale,\n",
    "            embedding_text_labels_norm=self.embedding_text_labels_norm, reduction=self.reduction\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig.to(main_device)\n",
    "model.to(main_device)\n",
    "model_orig.eval()\n",
    "model.train()\n",
    "\n",
    "# for x in locals().values():\n",
    "#     if isinstance(x, str) and x in ['True', 'False']:\n",
    "#         assert False, f'args contains a string that should be a bool: {x}'\n",
    "\n",
    "\n",
    "for i, (data, targets) in enumerate(dataloader):\n",
    "        # is_classification = isinstance(targets, torch.Tensor)\n",
    "        data = data.to(main_device)\n",
    "        n_samples = data.shape[0]\n",
    "        # if is_classification:\n",
    "        targets = targets.to(main_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding_orig = model_orig(vision=data, output_normalize=output_normalize)\n",
    "\n",
    "        # loss for the attack\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        loss_inner_wrapper = ComputeLossWrapper(\n",
    "            embedding_orig, embedding_text_labels_norm,\n",
    "            reduction='none' if attack == 'apgd' else 'mean', loss=inner_loss,\n",
    "            logit_scale=100.\n",
    "            )\n",
    "        \n",
    "        data_adv = apgd(\n",
    "                model=model,\n",
    "                loss_fn=loss_inner_wrapper,\n",
    "                x=data,\n",
    "                y=targets,\n",
    "                norm=norm,\n",
    "                eps=eps,\n",
    "                n_iter=iterations_adv,\n",
    "                verbose=True\n",
    "            )\n",
    "        del loss_inner_wrapper\n",
    "        print(data_adv)\n",
    "        print(data_adv.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robustVLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
